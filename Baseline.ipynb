{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducing Calvano et al. (2020)\n",
    "## Baseline\n",
    "### Author: Andréa Epivent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from copy import deepcopy\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our custom functions\n",
    "os.chdir(\"/Users/admin/Desktop/ENSAE/3A/Mémoire/Codes/Functions\")\n",
    "import find_state\n",
    "import profitquantity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "m = 15\n",
    "ci = 1\n",
    "ai = 2\n",
    "a0 = 0\n",
    "mu = 1/4\n",
    "delta = 0.95\n",
    "m = 15\n",
    "epsilon = 0.1\n",
    "k = 1 # memory width \n",
    "p_N = 1.47 # has to be solved numerically\n",
    "p_M = 1.92 # has to be solved numerically\n",
    "\n",
    "state_space = m**(n*k)\n",
    "action_space = m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15,)\n"
     ]
    }
   ],
   "source": [
    "# m equally spaced points in an interval that includes Nash and monopoly prices\n",
    "A = np.linspace(p_N-epsilon*(p_M-p_N),p_M+epsilon*(p_M-p_N),m)\n",
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* State space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combination of prices from last period\n",
    "S = np.zeros([state_space, 2])\n",
    "l = 0\n",
    "for i in A:\n",
    "    for j in A:\n",
    "        S[l,0] = i\n",
    "        S[l,1] = j\n",
    "        l += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/Users/admin/Desktop/ENSAE/3A/Mémoire/Codes/Output/Baseline/actions', A)\n",
    "np.save('/Users/admin/Desktop/ENSAE/3A/Mémoire/Codes/Output/Baseline/states', S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Q-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(225, 15)\n"
     ]
    }
   ],
   "source": [
    "# We start with zeros\n",
    "q_table = np.zeros([state_space, action_space])\n",
    "print(q_table.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_Q()\n",
    "\n",
    "b = 0 # loop over column\n",
    "sum_profit = 0\n",
    "for l in range(state_space):\n",
    "    for i in A:\n",
    "        for j in A:\n",
    "            profit = profitquantity.profit_compute(i,j,ci,ai,mu,a0)\n",
    "            sum_profit += profit\n",
    "        denom = (1-delta)*(action_space**(n-1))\n",
    "        q_table[l,b] = sum_profit/denom\n",
    "        sum_profit = 0\n",
    "        b += 1\n",
    "    b = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1314600\n",
      "Episode: 99\n",
      "Process has converged\n",
      "Training finished, episode: 99\n",
      "CPU times: user 7h 4min 34s, sys: 55min 35s, total: 8h 9s\n",
      "Wall time: 8h 13min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"Training the agent\"\"\"\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.15 # learning rate - baseline scenario\n",
    "beta = 4*10**-6 # experimentation parameter - baseline scenario\n",
    "\n",
    "# Stop criterion\n",
    "criterion = 10**4\n",
    "\n",
    "# Stop in any case\n",
    "criterion_final = 15*10**5 \n",
    "\n",
    "# Number of episodes\n",
    "n_episodes = 100\n",
    "\n",
    "# Store info in array\n",
    "q_info = np.zeros((criterion_final,4*n_episodes))\n",
    "\n",
    "# Initial Q_tables\n",
    "q_tables1 = np.zeros([state_space, action_space])\n",
    "q_tables2 = np.zeros([state_space, action_space])\n",
    "\n",
    "for j in range(n_episodes):\n",
    "\n",
    "    # Initialize Q-tables\n",
    "    q_table_a1 = deepcopy(q_table)\n",
    "    q_table_a2 = deepcopy(q_table)\n",
    "\n",
    "    # Stop criterion\n",
    "    count_a1 = 0\n",
    "    count_a2 = 0\n",
    "\n",
    "    # The initial state is picked randomly\n",
    "    state = random.randint(0, state_space-1)\n",
    "\n",
    "    # Store initial state in dataframe\n",
    "    q_info[0,j*4] = S[state][0]\n",
    "    q_info[0,(j*4)+1] = S[state][1]\n",
    "\n",
    "    # Initialize matrix for keeping track of argmax_p q\n",
    "    stab1 = np.full([state_space],-1)\n",
    "    stab2 = np.full([state_space],-1)\n",
    "\n",
    "    # Initialize convergence\n",
    "    convergence = False\n",
    "\n",
    "    # Start iteration\n",
    "    i = 1    \n",
    "\n",
    "    # While we didn't reach convergence\n",
    "    while convergence == False:\n",
    "\n",
    "        # Time-declining exploration rate\n",
    "        epsilon = np.exp(-beta*i) # greedy parameter \n",
    "\n",
    "        # trade-off for agent 1\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action_a1 = random.randint(0, action_space-1) # Explore action space\n",
    "        else:\n",
    "            action_a1 = np.argmax(q_table_a1[state]) # Exploit learned values\n",
    "\n",
    "        # trade-off for agent 2\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action_a2 = random.randint(0, action_space-1) # Explore action space\n",
    "        else:\n",
    "            action_a2 = np.argmax(q_table_a2[state]) # Exploit learned values\n",
    "\n",
    "        # Retrieve new prices and next state\n",
    "        p1, p2 = A[action_a1], A[action_a2]\n",
    "        next_state = find_state.find_rowindex(S,p1,p2) # We find the row index associated with these two new prices\n",
    "\n",
    "        # Rewards\n",
    "        reward_a1 = profitquantity.profit_compute(p1,p2,ci,ai,mu,a0)\n",
    "        reward_a2 = profitquantity.profit_compute(p2,p1,ci,ai,mu,a0)\n",
    "\n",
    "        # Store in dataframe - We begin at i = 1\n",
    "        q_info[i,j*4] = p1\n",
    "        q_info[i,(j*4)+1] = p2\n",
    "        q_info[i,(j*4)+2] = reward_a1\n",
    "        q_info[i,(j*4)+3] = reward_a2\n",
    "\n",
    "        # Convergence\n",
    "        a1_argmax = np.argmax(q_table_a1[state]) \n",
    "        a2_argmax = np.argmax(q_table_a2[state]) \n",
    "\n",
    "        if a1_argmax == stab1[state]:\n",
    "            count_a1 += 1\n",
    "        else:\n",
    "            count_a1 = 0\n",
    "            stab1[state] = a1_argmax # reinitialization\n",
    "\n",
    "        if a2_argmax == stab2[state]:\n",
    "            count_a2 += 1\n",
    "        else:\n",
    "            count_a2 = 0\n",
    "            stab2[state] = a2_argmax\n",
    "\n",
    "        if (count_a1 >= criterion) & (count_a2 >= criterion):\n",
    "            convergence = True\n",
    "\n",
    "        # Updating Q-table\n",
    "        # Agent 1\n",
    "        old_value_a1 = q_table_a1[state, action_a1]\n",
    "        next_max_a1 = np.max(q_table_a1[next_state])\n",
    "\n",
    "        new_value_a1 = (1 - alpha) * old_value_a1 + alpha * (reward_a1 + delta * next_max_a1)\n",
    "        q_table_a1[state, action_a1] = new_value_a1\n",
    "\n",
    "        # Agent 2\n",
    "        old_value_a2 = q_table_a2[state, action_a2]\n",
    "        next_max_a2 = np.max(q_table_a2[next_state])\n",
    "\n",
    "        new_value_a2 = (1 - alpha) * old_value_a2 + alpha * (reward_a2 + delta * next_max_a2)\n",
    "        q_table_a2[state, action_a2] = new_value_a2\n",
    "\n",
    "        # Go to next step\n",
    "        state = next_state\n",
    "\n",
    "        # Display number of episodes and status of convergence\n",
    "        if i % 100 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Iteration: {i}\")\n",
    "            print(f\"Episode: {j}\")\n",
    "\n",
    "        if (i < criterion_final) & (convergence == True):\n",
    "            print(\"Process has converged\")\n",
    "\n",
    "        # If we didn't convergence after a certain threshold, we end the loop anyway\n",
    "        if (i == criterion_final-1) & (convergence == False):\n",
    "            print(\"Process has not converged\") \n",
    "            convergence = True\n",
    "\n",
    "        i += 1\n",
    "        \n",
    "    # Save every Q-table\n",
    "    q_tables1 = np.concatenate((q_tables1,q_table_a1))\n",
    "    q_tables2 = np.concatenate((q_tables2,q_table_a2))\n",
    "\n",
    "    print(f\"Training finished, episode: {j}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results\n",
    "# Information on prices and profits for both agents\n",
    "np.save('/Users/admin/Desktop/ENSAE/3A/Mémoire/Codes/Output/Baseline/q_info', q_info)\n",
    "\n",
    "# Last Q matrix of both agents\n",
    "np.save('/Users/admin/Desktop/ENSAE/3A/Mémoire/Codes/Output/Baseline/q_table_a1', q_tables1)\n",
    "np.save('/Users/admin/Desktop/ENSAE/3A/Mémoire/Codes/Output/Baseline/q_table_a2', q_tables2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_info = np.load('/Users/admin/Desktop/ENSAE/3A/Mémoire/Codes/Output/Baseline/q_info.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with column names\n",
    "col = [\"p1_ep\",\"p2_ep\",\"profit1_ep\",\"profit2_ep\"]\n",
    "newcol = []\n",
    "for i in range(n_episodes):\n",
    "    for j in col:\n",
    "        newcol.append(j+str(i+1))\n",
    "        \n",
    "# Create dataframe\n",
    "q_df = pd.DataFrame(q_info, columns=newcol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1141930.59"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find convergence threshold\n",
    "for i in range(n_episodes):\n",
    "    name = \"convergence_ep\"+str(i+1)\n",
    "    if q_df['p1_ep'+str(i+1)].iloc[criterion_final-1] != 0:\n",
    "        q_df[name] = criterion_final\n",
    "    else:\n",
    "        q_df[name] = q_df.index[q_df['p1_ep'+str(i+1)]==0][0]\n",
    "\n",
    "convergence = np.array(q_df.iloc[0,400:500])\n",
    "convergence.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7790857142857142\n",
      "1.773685714285714\n"
     ]
    }
   ],
   "source": [
    "# Stock last prices\n",
    "price1 = []\n",
    "price2 = []\n",
    "for i in range(n_episodes):\n",
    "    price1.append(q_df[\"p1_ep\"+str(i+1)].iloc[q_df[\"convergence_ep\"+str(i+1)].iloc[0]-1])\n",
    "    price2.append(q_df[\"p2_ep\"+str(i+1)].iloc[q_df[\"convergence_ep\"+str(i+1)].iloc[0]-1])\n",
    "    \n",
    "print(np.mean(price1))\n",
    "print(np.mean(price2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute extra-profit gains - more efficiently\n",
    "profit_N = profitquantity.profit_compute(p_N,p_N,ci,ai,mu,a0)\n",
    "profit_M = profitquantity.profit_compute(p_M,p_M,ci,ai,mu,a0)\n",
    "\n",
    "extra_profit = []\n",
    "for i in range(len(price1)):\n",
    "    extra_profit.append((((profitquantity.profit_compute(price1[i],price2[i],ci,ai,mu,a0) + profitquantity.profit_compute(price2[i],price1[i],ci,ai,mu,a0))/2)-profit_N)/(profit_M-profit_N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as numpy format for later use\n",
    "np.save('/Users/admin/Desktop/ENSAE/3A/Mémoire/Codes/Output/Baseline/convergence', convergence)\n",
    "np.save('/Users/admin/Desktop/ENSAE/3A/Mémoire/Codes/Output/Baseline/price1', np.array(price1))\n",
    "np.save('/Users/admin/Desktop/ENSAE/3A/Mémoire/Codes/Output/Baseline/price2', np.array(price2))\n",
    "np.save('/Users/admin/Desktop/ENSAE/3A/Mémoire/Codes/Output/Baseline/extra_profit', np.array(extra_profit))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
