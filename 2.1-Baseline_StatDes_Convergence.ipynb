{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducing Calvano et al. (2020)\n",
    "## Baseline - Stat des - Convergence\n",
    "### Author: Andr√©a Epivent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory\n",
    "path = \"/Users/admin/Desktop/PhD/RL_AlgorithmicCollusion\"\n",
    "\n",
    "# Import packages and custom functions\n",
    "exec(open(path+\"/Functions/import.py\").read())\n",
    "\n",
    "# Import parameters\n",
    "exec(open(path+\"/Functions/parameters.py\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data from training\n",
    "q_table_1 = np.load(path+'/Output/Baseline/q_table_a1.npy')\n",
    "q_table_2 = np.load(path+'/Output/Baseline/q_table_a2.npy')\n",
    "q_info = np.load(path+'/Output/Baseline/q_info.npy')\n",
    "A = np.load(path+'/Output/Baseline/actions.npy')\n",
    "S = np.load(path+'/Output/Baseline/states.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve number of iterations for each episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = np.zeros((n_episodes,))\n",
    "for j in range(n_episodes):\n",
    "    if q_info[criterion_final-1,j*4] != 0:\n",
    "        n_iterations[j] = criterion_final\n",
    "    else:\n",
    "        n_iterations[j] = np.where(q_info[:,j*4] == 0)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for later use\n",
    "np.save(path+'/Output/Baseline/n_iterations', n_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Average number of iterations per episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1141930.59"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_iterations.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Number of iterations that did not converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(n_iterations == criterion_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Idea</b>: check that optimal actions don't change when we turn exploration off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 1 changed optimal action at stage 223 and period 3\n",
      "Agent 2 changed optimal action at stage 223 and period 3\n",
      "Agent 1 changed optimal action at stage 58 and period 3\n",
      "Agent 2 changed optimal action at stage 58 and period 3\n",
      "Agent 1 changed optimal action at stage 111 and period 3\n",
      "Agent 2 changed optimal action at stage 111 and period 3\n",
      "Agent 1 changed optimal action at stage 41 and period 3\n",
      "Agent 2 changed optimal action at stage 41 and period 3\n",
      "Agent 1 changed optimal action at stage 146 and period 3\n",
      "Agent 2 changed optimal action at stage 146 and period 3\n",
      "Agent 1 changed optimal action at stage 207 and period 3\n",
      "Agent 2 changed optimal action at stage 207 and period 3\n",
      "Agent 1 changed optimal action at stage 221 and period 3\n",
      "Agent 2 changed optimal action at stage 221 and period 3\n",
      "Agent 1 changed optimal action at stage 174 and period 3\n",
      "Agent 2 changed optimal action at stage 174 and period 3\n",
      "Agent 1 changed optimal action at stage 218 and period 3\n",
      "Agent 2 changed optimal action at stage 218 and period 3\n"
     ]
    }
   ],
   "source": [
    "# Stock last price for both agents for each episode\n",
    "#f_price1, f_price2 = get_forward_price(30,q_table_1,q_table_2,q_info,n_iterations,S,A)\n",
    "#price1 = f_price1[:,f_price1.shape[1]-1:f_price1.shape[1]]\n",
    "#price2 = f_price2[:,f_price2.shape[1]-1:f_price2.shape[1]]\n",
    "price1, price2 = get_last_price(1,q_info,n_iterations)\n",
    "threshold = 1e4\n",
    "conv_episodes = []\n",
    "\n",
    "for j in range(n_episodes): \n",
    "    \n",
    "    t = 0\n",
    "    \n",
    "    # Import q-matrix of both agents\n",
    "    q1 = q_table_1[(j+1)*225:(j+1)*225+225,:]\n",
    "    q2 = q_table_2[(j+1)*225:(j+1)*225+225,:]\n",
    "        \n",
    "    # Find last state and optimal action response according to limit strategy\n",
    "    state = find_rowindex(S,price1[j][0],price2[j][0])\n",
    "        \n",
    "    # Initialize convergence criteria\n",
    "    convergence = False\n",
    "    \n",
    "    # Initialize matrix for keeping track of argmax_p q\n",
    "    stab1 = np.full([state_space],-1)\n",
    "    stab2 = np.full([state_space],-1)\n",
    "\n",
    "    while convergence == False:\n",
    "\n",
    "        # Find optimal actions and keep track\n",
    "        action_a1 = np.argmax(q1[state])\n",
    "        action_a2 = np.argmax(q2[state])\n",
    "        \n",
    "        # Retrieve prices and next state\n",
    "        p1,p2 = A[action_a1], A[action_a2]\n",
    "        next_state = find_rowindex(S,p1,p2) # We find the row index associated with these two new prices\n",
    "\n",
    "        # Rewards\n",
    "        reward_a1 = profit_compute(p1,p2,ci,ai,mu,a0)\n",
    "        reward_a2 = profit_compute(p2,p1,ci,ai,mu,a0)\n",
    "\n",
    "        # Updating Q-table - for agent 1 \n",
    "        old_value_a1 = q1[state, action_a1]\n",
    "        next_max_a1 = np.max(q1[next_state])\n",
    "\n",
    "        new_value_a1 = (1 - alpha) * old_value_a1 + alpha * (reward_a1 + delta * next_max_a1)\n",
    "        q1[state, action_a1] = new_value_a1\n",
    "        \n",
    "        # Updating Q-table - for agent 2\n",
    "        old_value_a2 = q2[state, action_a2]\n",
    "        next_max_a2 = np.max(q2[next_state])\n",
    "\n",
    "        new_value_a2 = (1 - alpha) * old_value_a2 + alpha * (reward_a2 + delta * next_max_a2)\n",
    "        q2[state, action_a2] = new_value_a2\n",
    "\n",
    "        # We always stick to the same state\n",
    "        state = next_state\n",
    "\n",
    "        if (action_a1 != stab1[state]) & (t > 2):\n",
    "            convergence = True\n",
    "            print(f\"Agent 1 changed optimal action at stage {state} and period {t}\")\n",
    "            conv_episodes.append(False)\n",
    "        \n",
    "        if (action_a2 != stab2[state]) & (t > 2):\n",
    "            convergence = True\n",
    "            print(f\"Agent 2 changed optimal action at stage {state} and period {t}\")\n",
    "        \n",
    "        stab1[state] = action_a1\n",
    "        stab2[state] = action_a2\n",
    "        \n",
    "        t += 1\n",
    "\n",
    "        # Stop if optimal action changes\n",
    "        \n",
    "        # Stop in any case\n",
    "        if t == threshold:\n",
    "            convergence = True\n",
    "            conv_episodes.append(True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_episodes.count(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
