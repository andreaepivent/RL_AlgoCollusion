{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducing Calvano et al. (2020)\n",
    "## Baseline - Optimality\n",
    "### Author: Andr√©a Epivent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory\n",
    "path = \"/Users/admin/Desktop/PhD/RL_AlgorithmicCollusion\"\n",
    "\n",
    "# Import packages and custom functions\n",
    "exec(open(path+\"/Functions/import.py\").read())\n",
    "\n",
    "# Import parameters\n",
    "exec(open(path+\"/Functions/parameters.py\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data from training\n",
    "q_table_1 = np.load(path+'/Output/Baseline/q_table_a1.npy')\n",
    "q_table_2 = np.load(path+'/Output/Baseline/q_table_a2.npy')\n",
    "q_info = np.load(path+'/Output/Baseline/q_info.npy')\n",
    "A = np.load(path+'/Output/Baseline/actions.npy')\n",
    "S = np.load(path+'/Output/Baseline/states.npy')\n",
    "n_iterations = np.load(path+'/Output/Baseline/n_iterations.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing optimality measure for both agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve last prices for both agents\n",
    "price1, price2 = get_last_price(1,q_info,n_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Agent 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_a1 = []\n",
    "\n",
    "for j in range(n_episodes): \n",
    "    \n",
    "    q1 = q_table_1[(j+1)*225:(j+1)*225+225,:]\n",
    "    q2 = q_table_2[(j+1)*225:(j+1)*225+225,:]\n",
    "    \n",
    "    # Initialize Q-matrix of agent\n",
    "    q_table = init_Q(state_space,action_space,A,ci,ai,mu,a0,delta,n)\n",
    "        \n",
    "    # Find last state and optimal action response according to limit strategy\n",
    "    s_optim = find_rowindex(S,price1[j][0],price2[j][0])\n",
    "    a_optim = np.argmax(q1[s_optim])\n",
    "      \n",
    "    # Loop over every state/action until convergence\n",
    "    for act in A:\n",
    "        \n",
    "        # Initialize convergence criteria\n",
    "        count_convergence = 0\n",
    "        convergence = False\n",
    "\n",
    "        while convergence == False:\n",
    "\n",
    "            p1,p2 = act, A[np.argmax(q2[s_optim])] # Q-matrix of agent 2 doesn't change, play according to limit strategy\n",
    "            next_state = find_rowindex(S,p1,p2) # We find the row index associated with these two new prices\n",
    "            action = np.where(A == p1)[0][0] # get index associated to p1\n",
    "\n",
    "            # Rewards\n",
    "            reward = profit_compute(p1,p2,ci,ai,mu,a0)\n",
    "\n",
    "            # Updating Q-table - for agent 1 only\n",
    "            old_value = q_table[s_optim, action]\n",
    "            next_max = np.max(q_table[next_state])\n",
    "\n",
    "            new_value = (1 - alpha) * old_value + alpha * (reward + delta * next_max)\n",
    "            q_table[s_optim, action] = new_value\n",
    "\n",
    "            # We always stick to the same state\n",
    "            #state = next_state\n",
    "\n",
    "            diff = abs(old_value-new_value)\n",
    "\n",
    "            if diff < 1e-4:\n",
    "                count_convergence += 1\n",
    "            else:\n",
    "                count_convergence = 0\n",
    "\n",
    "            if count_convergence == 100: # doesn't change for at least 100 iterations\n",
    "                convergence = True\n",
    "                \n",
    "    optim_a1.append(q1[s_optim,a_optim]/np.max(q_table[s_optim])) # compare limit strategy and theoretical Q-matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Agent 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_a2 = []\n",
    "\n",
    "for j in range(n_episodes): \n",
    "    \n",
    "    q1 = q_table_1[(j+1)*225:(j+1)*225+225,:]\n",
    "    q2 = q_table_2[(j+1)*225:(j+1)*225+225,:]\n",
    "    \n",
    "    # Initialize Q-matrix of agent\n",
    "    q_table = init_Q(state_space,action_space,A,ci,ai,mu,a0,delta,n)\n",
    "        \n",
    "    # Find last state and optimal action response according to limit strategy\n",
    "    s_optim = find_rowindex(S,price1[j][0],price2[j][0])\n",
    "    a_optim = np.argmax(q2[s_optim])\n",
    "      \n",
    "    # Loop over every state/action until convergence\n",
    "    for act in A:\n",
    "        \n",
    "        # Initialize convergence criteria\n",
    "        count_convergence = 0\n",
    "        convergence = False\n",
    "\n",
    "        while convergence == False:\n",
    "\n",
    "            p1,p2 = A[np.argmax(q1[s_optim])], act # Q-matrix of agent 1 doesn't change, play according to limit strategy\n",
    "            next_state = find_rowindex(S,p1,p2) # We find the row index associated with these two new prices\n",
    "            action = np.where(A == p2)[0][0] # get index associated to p2\n",
    "\n",
    "            # Rewards\n",
    "            reward = profit_compute(p2,p1,ci,ai,mu,a0)\n",
    "\n",
    "            # Updating Q-table - for agent 2 only\n",
    "            old_value = q_table[s_optim, action]\n",
    "            next_max = np.max(q_table[next_state])\n",
    "\n",
    "            new_value = (1 - alpha) * old_value + alpha * (reward + delta * next_max)\n",
    "            q_table[s_optim, action] = new_value\n",
    "\n",
    "            # We always stick to the same state\n",
    "            #state = next_state\n",
    "\n",
    "            diff = abs(old_value-new_value)\n",
    "\n",
    "            if diff < 1e-4:\n",
    "                count_convergence += 1\n",
    "            else:\n",
    "                count_convergence = 0\n",
    "\n",
    "            if count_convergence == 100: # doesn't change for at least 100 iterations\n",
    "                convergence = True\n",
    "                \n",
    "    optim_a2.append(q2[s_optim,a_optim]/np.max(q_table[s_optim])) # compare limit strategy and theoretical Q-matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9889157639115969"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(optim_a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fraction of Nash equilibrium played by agent 1\n",
    "a1_t1 = np.where(abs(np.array(optim_a1)-1)<0.001)[0]\n",
    "a1_t2 = np.where(abs(np.array(optim_a1)-1)<0.01)[0]\n",
    "a1_t3 = np.where(abs(np.array(optim_a1)-1)<0.05)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9865311879646961"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(optim_a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fraction of Nash equilibrium played by agent 2\n",
    "a2_t1 = np.where(abs(np.array(optim_a2)-1)<0.001)[0]\n",
    "a2_t2 = np.where(abs(np.array(optim_a2)-1)<0.01)[0]\n",
    "a2_t3 = np.where(abs(np.array(optim_a2)-1)<0.05)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fraction of Nash equilibrium played by both agents\n",
    "a1_a2_t1 = np.intersect1d(a1_t1,a2_t1)\n",
    "a1_a2_t2 = np.intersect1d(a1_t2,a2_t2)\n",
    "a1_a2_t3 = np.intersect1d(a1_t3,a2_t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9889157639115969 (4,) (26,) (76,)\n",
      "0.9865311879646961 (3,) (24,) (77,)\n",
      "0.9877234759381466 (0,) (6,) (61,)\n"
     ]
    }
   ],
   "source": [
    "# Descriptive table\n",
    "print(np.mean(optim_a1),a1_t1.shape,a1_t2.shape,a1_t3.shape)\n",
    "print(np.mean(optim_a2),a2_t1.shape,a2_t2.shape,a2_t3.shape)\n",
    "print(np.mean(np.concatenate((optim_a1,optim_a2))),a1_a2_t1.shape,a1_a2_t2.shape,a1_a2_t3.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
